"""
ë§¥ë½ ê¸°ë°˜ ëŒ€í™” ë¶„ì„ ì‹œìŠ¤í…œ
- VTT íŒŒì¼ì˜ ì˜ë¯¸ì  ë¶„ì„
- AI ê¸°ë°˜ ëŒ€í™” ë‚´ìš© ìš”ì•½ ë° ë°œë‹¬ì  ìˆœê°„ ì¶”ì¶œ
- ê³„ì¸µì  ìš”ì•½ (ì²­í¬ â†’ ì „ì²´)
"""

import os
import json
import re
from typing import List, Dict, Any
from datetime import datetime


class ContextualDialogueAnalyzer:
    """ë§¥ë½ ê¸°ë°˜ ëŒ€í™” ë¶„ì„ê¸°"""
    
    def __init__(self, session_path: str):
        """
        Args:
            session_path: ì„¸ì…˜ í´ë” ê²½ë¡œ
        """
        self.session_path = session_path
        self.session_name = os.path.basename(session_path)
        self.vtt_path = os.path.join(session_path, 'vtt')
        
        # ë©”íƒ€ë°ì´í„° íŒŒì‹±
        self.metadata = self._parse_session_name()
        
        # ëŒ€í™” ë°ì´í„°
        self.dialogues = []
        self.chunk_summaries = []
        self.final_summary = {}
    
    def _parse_session_name(self) -> Dict[str, str]:
        """ì„¸ì…˜ ì´ë¦„ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        parts = self.session_name.split('-')
        return {
            'date': parts[0] if len(parts) > 0 else '',
            'teacher': parts[1].replace('êµì‚¬', '') if len(parts) > 1 else '',
            'child': parts[2] if len(parts) > 2 else '',
            'age': parts[3] if len(parts) > 3 else '',
        }
    
    def load_vtt_files(self, chunk_minutes: int = 10) -> List[Dict[str, Any]]:
        """
        VTT íŒŒì¼ë“¤ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ ë¡œë“œ
        
        Args:
            chunk_minutes: ì²­í¬ í¬ê¸° (ë¶„ ë‹¨ìœ„)
        
        Returns:
            ì²­í¬ë³„ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“‚ VTT íŒŒì¼ ë¡œë“œ ì¤‘: {self.vtt_path}")
        
        # VTT íŒŒì¼ ëª©ë¡ (í›„ì²˜ë¦¬ëœ íŒŒì¼ ìš°ì„ )
        vtt_files = []
        for filename in sorted(os.listdir(self.vtt_path)):
            if filename.endswith('.vtt'):
                # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: 010-012ë¶„)
                time_match = re.search(r'(\d{3})-(\d{3})ë¶„', filename)
                if time_match:
                    start_min = int(time_match.group(1))
                    end_min = int(time_match.group(2))
                    
                    # ìš°ì„ ìˆœìœ„: í›„ì²˜ë¦¬ë¨ > subtitle > ë¯¸ì²˜ë¦¬
                    if '_í›„ì²˜ë¦¬ë¨' in filename:
                        priority = 3
                    elif '_subtitle.vtt' in filename and '_ë¯¸ì²˜ë¦¬' not in filename and '_í›„ì²˜ë¦¬ë¨' not in filename:
                        priority = 2
                    elif '_ë¯¸ì²˜ë¦¬' in filename:
                        priority = 1
                    else:
                        priority = 2  # ê¸°ë³¸ê°’
                    
                    vtt_files.append({
                        'filename': filename,
                        'start_min': start_min,
                        'end_min': end_min,
                        'priority': priority
                    })
        
        # ì¤‘ë³µ ì œê±° (í›„ì²˜ë¦¬ ìš°ì„ )
        time_slots = {}
        for vtt in vtt_files:
            key = (vtt['start_min'], vtt['end_min'])
            if key not in time_slots or vtt['priority'] > time_slots[key]['priority']:
                time_slots[key] = vtt
        
        selected_files = sorted(time_slots.values(), key=lambda x: x['start_min'])
        
        # ì²­í¬ë¡œ ê·¸ë£¹í™”
        chunks = []
        current_chunk = {
            'start_min': 0,
            'end_min': chunk_minutes,
            'dialogues': []
        }
        
        for vtt in selected_files:
            filepath = os.path.join(self.vtt_path, vtt['filename'])
            
            # VTT íŒŒì¼ ì½ê¸°
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ëŒ€í™” ì¶”ì¶œ
            dialogues = self._parse_vtt_content(content)
            
            # ì²­í¬ì— ì¶”ê°€
            if vtt['start_min'] < current_chunk['end_min']:
                current_chunk['dialogues'].extend(dialogues)
            else:
                # ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk['dialogues']:
                    chunks.append(current_chunk)
                current_chunk = {
                    'start_min': vtt['start_min'],
                    'end_min': vtt['start_min'] + chunk_minutes,
                    'dialogues': dialogues
                }
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk['dialogues']:
            chunks.append(current_chunk)
        
        print(f"âœ… {len(selected_files)}ê°œ íŒŒì¼ì„ {len(chunks)}ê°œ ì²­í¬ë¡œ ê·¸ë£¹í™”")
        
        return chunks
    
    def _parse_vtt_content(self, content: str) -> List[Dict[str, Any]]:
        """VTT ë‚´ìš© íŒŒì‹±"""
        dialogues = []
        
        pattern = r'(\d{2}:\d{2}:\d{2}\.\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}\.\d{3})\s*\n\[(.*?)\]\s*(.*?)(?=\n\n|\Z)'
        matches = re.finditer(pattern, content, re.DOTALL)
        
        for match in matches:
            speaker = match.group(3).strip()
            text = match.group(4).strip()
            
            speaker_type = 'teacher' if 'ì„ ìƒë‹˜' in speaker or 'êµì‚¬' in speaker else 'child'
            
            dialogues.append({
                'speaker': speaker,
                'speaker_type': speaker_type,
                'text': text
            })
        
        return dialogues
    
    def create_chunk_prompt(self, chunk: Dict[str, Any]) -> str:
        """ì²­í¬ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        dialogues_text = []
        for d in chunk['dialogues']:
            speaker_label = "ì„ ìƒë‹˜" if d['speaker_type'] == 'teacher' else "ì•„ì´"
            dialogues_text.append(f"[{speaker_label}] {d['text']}")
        
        prompt = f"""ë‹¤ìŒì€ {self.metadata['teacher']} ì„ ìƒë‹˜ê³¼ {self.metadata['child']} ì•„ë™({self.metadata['age']})ì˜ 
{chunk['start_min']}-{chunk['end_min']}ë¶„ êµ¬ê°„ ë†€ì´ ëŒ€í™”ì…ë‹ˆë‹¤.

# ëŒ€í™” ë‚´ìš©
{chr(10).join(dialogues_text[:200])}  # ìµœëŒ€ 200ê°œ ë°œí™”ë§Œ

# ë¶„ì„ ìš”ì²­
ë‹¤ìŒ ê´€ì ì—ì„œ ì´ êµ¬ê°„ì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **ì£¼ìš” ë†€ì´ í™œë™**: ì–´ë–¤ ë†€ì´ë¥¼ í–ˆë‚˜ìš”?
2. **ì˜ë¯¸ìˆëŠ” ìƒí˜¸ì‘ìš©**: êµìœ¡ì ìœ¼ë¡œ ì¤‘ìš”í•œ ìˆœê°„ì€?
3. **ì–¸ì–´ ë°œë‹¬ ìˆœê°„**: íŠ¹ë³„í•œ ì–¸ì–´ ì‚¬ìš©, ì–´íœ˜ í•™ìŠµ, ë¬¸ì¥ êµ¬ì¡° ë°œë‹¬
4. **ì¸ì§€ ë°œë‹¬ ìˆœê°„**: ë¬¸ì œí•´ê²°, íƒêµ¬, ì§ˆë¬¸, ì¶”ë¡ 
5. **ì •ì„œ ì‚¬íšŒì„±**: ê°ì • í‘œí˜„, í˜‘ë ¥, ê°ˆë“± í•´ê²°
6. **í•µì‹¬ ì—í”¼ì†Œë“œ**: ê°€ì¥ ì¸ìƒì ì¸ ì¥ë©´ 1-2ê°œ (êµ¬ì²´ì  ëŒ€í™” ì¸ìš©)

ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš” (300ì ì´ë‚´).
"""
        
        return prompt
    
    def analyze_chunk_with_ai(self, chunk: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì²­í¬ë¥¼ AIë¡œ ë¶„ì„ (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ)
        
        í˜„ì¬ëŠ” í”„ë¡¬í”„íŠ¸ë§Œ ìƒì„±í•˜ê³ , ì‹¤ì œ API í˜¸ì¶œì€ ë³„ë„ êµ¬í˜„ í•„ìš”
        """
        prompt = self.create_chunk_prompt(chunk)
        
        # TODO: ì‹¤ì œ Claude API í˜¸ì¶œ
        # response = anthropic.messages.create(...)
        
        # ì§€ê¸ˆì€ êµ¬ì¡°ë§Œ ë°˜í™˜
        return {
            'time_range': f"{chunk['start_min']}-{chunk['end_min']}ë¶„",
            'prompt': prompt,
            'dialogue_count': len(chunk['dialogues']),
            # 'ai_summary': response.content  # ì‹¤ì œ API ì‘ë‹µ
        }
    
    def generate_prompts_for_manual_analysis(self, output_dir: str = 'contextual_prompts'):
        """
        ìˆ˜ë™ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ìƒì„±
        (API í‚¤ ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
        """
        os.makedirs(output_dir, exist_ok=True)
        
        chunks = self.load_vtt_files(chunk_minutes=10)
        
        prompts = []
        for i, chunk in enumerate(chunks, 1):
            analysis = self.analyze_chunk_with_ai(chunk)
            prompts.append(analysis)
            
            # ê°œë³„ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì €ì¥
            prompt_file = os.path.join(
                output_dir, 
                f"{self.session_name}_chunk{i:02d}_{analysis['time_range']}.txt"
            )
            
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(analysis['prompt'])
                f.write("\n\n" + "="*70)
                f.write(f"\në°œí™” ìˆ˜: {analysis['dialogue_count']}ê°œ")
        
        # í†µí•© ìš”ì•½ë³¸
        summary_file = os.path.join(output_dir, f"{self.session_name}_all_prompts.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'session': self.session_name,
                'metadata': self.metadata,
                'total_chunks': len(prompts),
                'prompts': prompts
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… {len(prompts)}ê°œ ì²­í¬ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“ ì¶œë ¥ í´ë”: {output_dir}")
        print(f"\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        print(f"   1. ê° í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ Claude/GPTì— ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥")
        print(f"   2. AI ì‘ë‹µì„ ë°›ì•„ì„œ ì €ì¥")
        print(f"   3. ëª¨ë“  ìš”ì•½ì„ í†µí•©í•˜ì—¬ ìµœì¢… ë¶„ì„")
        
        return prompts


def analyze_session_contextually(session_path: str, output_dir: str = 'contextual_prompts'):
    """ì„¸ì…˜ì˜ ë§¥ë½ì  ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ ë§¥ë½ ê¸°ë°˜ ë¶„ì„ ì‹œì‘: {os.path.basename(session_path)}")
    print(f"{'='*70}\n")
    
    analyzer = ContextualDialogueAnalyzer(session_path)
    prompts = analyzer.generate_prompts_for_manual_analysis(output_dir)
    
    print(f"\n{'='*70}")
    print(f"âœ… ì™„ë£Œ!")
    print(f"{'='*70}\n")
    
    return prompts


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ - 2ì‹œê°„ ì„¸ì…˜
    session_path = '/Users/healin/Downloads/develop/care-intell/raw_data/20251017-ì´ë¯¼ì •êµì‚¬-ê¹€ì¤€ìš°-ë§Œ4ì„¸-02_00_48-65kbps_mono'
    
    prompts = analyze_session_contextually(session_path)
    
    print(f"\nğŸ“Š ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {len(prompts)}ê°œ")
    
    if prompts:
        print(f"\nì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°:")
        print("=" * 70)
        print(prompts[0]['prompt'][:500] + "...")
    else:
        print("\nâš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

